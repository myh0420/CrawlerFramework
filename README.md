CrawlerFramework/
â”œâ”€â”€ CrawlerCore/                      // æ ¸å¿ƒçˆ¬è™«å¼•æ“
â”œâ”€â”€ CrawlerScheduler/                 // ä»»åŠ¡è°ƒåº¦å™¨
â”œâ”€â”€ CrawlerDownloader/                // ä¸‹è½½ç»„ä»¶
â”œâ”€â”€ CrawlerParser/                    // å†…å®¹è§£æå™¨
â”œâ”€â”€ CrawlerStorage/                   // å­˜å‚¨æ¨¡å—
â”œâ”€â”€ CrawlerMonitor/                   // ç›‘æ§ç•Œé¢
â”œâ”€â”€ CrawlerConsole/                   // æ§åˆ¶å°åº”ç”¨
â”œâ”€â”€ CrawlerInterFaces/                // æ¥å£å®šä¹‰
â”œâ”€â”€ CrawlerEntity/                    // å®ä½“æ¨¡å‹
â””â”€â”€ CrawlerServiceDependencyInjection/ // ä¾èµ–æ³¨å…¥æœåŠ¡

# ç½‘ç»œçˆ¬è™«æ¡†æ¶ä½¿ç”¨è¯´æ˜å’ŒåŠŸèƒ½ç¤ºä¾‹

## 1. æ¡†æ¶æ¦‚è¿°


æœ¬çˆ¬è™«æ¡†æ¶æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€å¯æ‰©å±•çš„ä¼ä¸šçº§ç½‘ç»œçˆ¬è™«è§£å†³æ–¹æ¡ˆï¼Œé›†æˆäº†å¤šç§çˆ¬è™«å·¥å…·çš„ä¼˜ç‚¹ï¼Œæä¾›äº†é«˜æ€§èƒ½ã€å¯é æ€§å’Œæ˜“ç”¨æ€§ã€‚

### ä¸»è¦ç‰¹æ€§
- ğŸš€ **é«˜æ€§èƒ½**ï¼šå¼‚æ­¥å¹¶å‘å¤„ç†ï¼Œè¿æ¥å¤ç”¨ï¼Œå†…å­˜ä¼˜åŒ–ï¼Œç¼–è¯‘æ—¶æ­£åˆ™è¡¨è¾¾å¼
- ğŸ›¡ï¸ **å¯é æ€§**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶ã€éµå®ˆrobots.txt
- ğŸ”§ **å¯é…ç½®**ï¼šçµæ´»çš„é…ç½®ç³»ç»Ÿã€å¯æ’æ‹”ç»„ä»¶
- ğŸ“Š **ç›‘æ§**ï¼šï¼ˆéƒ¨åˆ†å®ç°ï¼‰åŸºç¡€æ¶æ„æ­å»ºï¼ŒWebç•Œé¢æ¡†æ¶ï¼ˆå¼€å‘ä¸­ï¼‰
- ğŸ”Œ **å¯æ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°åŠŸèƒ½
- ğŸ“ **å†…å®¹è§£æ**ï¼šæ”¯æŒHTMLã€çº¯æ–‡æœ¬ã€JSONç­‰å¤šç§å†…å®¹ç±»å‹ï¼Œå®‰å…¨çš„HTMLæ–‡æ¡£å¤„ç†
- â±ï¸ **åŸŸåè¯·æ±‚èŠ‚æµ**ï¼šåŸºäºåŸŸåçš„åŠ¨æ€å»¶è¿Ÿè°ƒæ•´ï¼Œæ”¯æŒè¯·æ±‚ç±»å‹æ„ŸçŸ¥èŠ‚æµ
- ğŸ“¡ **åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦**ï¼šæ”¯æŒå¤šèŠ‚ç‚¹éƒ¨ç½²ï¼Œé˜²æ­¢é‡å¤å¤„ç†
- ğŸ¯ **ä¼˜å…ˆçº§é˜Ÿåˆ—ä¼˜åŒ–**ï¼šåŸºäºæ·±åº¦ã€å†…å®¹ç±»å‹ã€åŸŸé‡è¦æ€§å’Œç­‰å¾…æ—¶é—´çš„æ™ºèƒ½ä¼˜å…ˆçº§è®¡ç®—
- ğŸ”„ **URLå½’ä¸€åŒ–**ï¼šï¼ˆè®¡åˆ’å®ç°ï¼‰URLæ ‡å‡†åŒ–å¤„ç†ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒå†…å®¹
- ğŸ“ˆ **åŠ¨æ€å»¶è¿Ÿè°ƒæ•´**ï¼šæ ¹æ®æœåŠ¡å™¨å“åº”è‡ªåŠ¨è°ƒæ•´å»¶è¿Ÿï¼Œé˜²æ­¢æœåŠ¡å™¨å‹åŠ›è¿‡å¤§

## 2. å¿«é€Ÿå¼€å§‹

### 2.1 åŸºæœ¬çˆ¬è™«ä½¿ç”¨

```csharp
// Program.cs
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using CrawlerCore;
using CrawlerServiceDependencyInjection.DependencyInjection;

class Program
{
    static async Task Main(string[] args)
    {
        // é…ç½®ä¾èµ–æ³¨å…¥
        var services = new ServiceCollection();
        services.AddLogging(builder => builder.AddConsole().SetMinimumLevel(LogLevel.Information));
        
        // æ·»åŠ çˆ¬è™«æœåŠ¡
        services.AddCrawlerCore()
                .AddFileSystemStorage("crawler_data")
                .AddCrawlerDownloader()
                .AddCrawlerParser()
                .AddCrawlerScheduler();

        var serviceProvider = services.BuildServiceProvider();
        var crawler = serviceProvider.GetRequiredService<CrawlerEngine>();

        // é…ç½®çˆ¬è™«
        var config = new CrawlConfiguration
        {
            MaxConcurrentTasks = 5,
            MaxDepth = 2,
            MaxPages = 100,
            RequestDelay = TimeSpan.FromMilliseconds(1000),
            AllowedDomains = new[] { "example.com" }
        };

        // æ³¨å†Œäº‹ä»¶
        crawler.OnCrawlCompleted += (s, e) => 
        {
            Console.WriteLine($"Completed: {e.Result.Request.Url}");
        };

        // å¯åŠ¨çˆ¬è™«
        await crawler.StartAsync(config);
        
        // æ·»åŠ ç§å­URL
        await crawler.AddSeedUrlsAsync(new[] 
        {
            "https://example.com",
            "https://example.com/news"
        });

        Console.WriteLine("Press any key to stop...");
        Console.ReadKey();

        await crawler.StopAsync();
    }
}
```

### 2.2 é¡¹ç›®æ–‡ä»¶é…ç½®

```xml
<!-- CrawlerConsole.csproj -->
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net9.0</TargetFramework>
    <ImplicitUsings>enable</ImplicitUsings>
    <Nullable>enable</Nullable>
  </PropertyGroup>

  <ItemGroup>
    <PackageReference Include="Microsoft.Extensions.DependencyInjection" Version="9.0.0" />
    <PackageReference Include="Microsoft.Extensions.Logging.Console" Version="9.0.0" />
  </ItemGroup>

  <ItemGroup>
    <ProjectReference Include="..\CrawlerCore\CrawlerCore.csproj" />
    <ProjectReference Include="..\CrawlerDownloader\CrawlerDownloader.csproj" />
    <ProjectReference Include="..\CrawlerParser\CrawlerParser.csproj" />
    <ProjectReference Include="..\CrawlerScheduler\CrawlerScheduler.csproj" />
    <ProjectReference Include="..\CrawlerStorage\CrawlerStorage.csproj" />
  </ItemGroup>

</Project>
```

## 3. æ ¸å¿ƒé…ç½®è¯¦è§£

æœ¬æ¡†æ¶æä¾›äº†å¼ºå¤§çš„é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§é…ç½®æ–¹å¼å’ŒåŠ¨æ€æ›´æ–°ï¼š

### é…ç½®ç³»ç»Ÿç‰¹æ€§
- **JSONé…ç½®æ–‡ä»¶**ï¼šæ‰€æœ‰é…ç½®éƒ½å­˜å‚¨åœ¨ appsettings.json ä¸­
- **Webé…ç½®ç•Œé¢**ï¼šï¼ˆéƒ¨åˆ†å®ç°ï¼‰åŸºç¡€ç•Œé¢æ¡†æ¶ï¼Œé…ç½®çƒ­é‡è½½åŠŸèƒ½
- **é…ç½®éªŒè¯**ï¼šå®æ—¶éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§
- **é…ç½®çƒ­é‡è½½**ï¼šæ”¯æŒé…ç½®æ–‡ä»¶å˜åŒ–è‡ªåŠ¨é‡è½½
- **é»˜è®¤å€¼ç®¡ç†**ï¼šæä¾›åˆç†çš„é»˜è®¤é…ç½®
- **ç±»å‹å®‰å…¨**ï¼šå¼ºç±»å‹é…ç½®æ¨¡å‹

### é…ç½®ä½¿ç”¨æ–¹å¼
- **å¯åŠ¨åº”ç”¨**ï¼šåº”ç”¨ä¼šè‡ªåŠ¨åŠ è½½ appsettings.json é…ç½®æ–‡ä»¶
- **è®¿é—®é…ç½®ç•Œé¢**ï¼šåŸºç¡€ç•Œé¢æ¡†æ¶å·²æ­å»ºï¼Œå®Œæ•´åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­
- **ä¿®æ”¹é…ç½®**ï¼šç›®å‰éœ€æ‰‹åŠ¨ç¼–è¾‘é…ç½®æ–‡ä»¶
- **ä¿å­˜é…ç½®**ï¼šé…ç½®æ–‡ä»¶å˜æ›´ä¼šè‡ªåŠ¨é‡è½½
- **éªŒè¯é…ç½®**ï¼šå®æ—¶éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§

### é…ç½®æ–‡ä»¶ä½ç½®
- **æ§åˆ¶å°åº”ç”¨**ï¼šCrawlerConsole/appsettings.json
- **Webç›‘æ§åº”ç”¨**ï¼šCrawlerMonitor/appsettings.json

### é…ç½®ç³»ç»Ÿä¼˜åŠ¿
- **æ— ç¡¬ç¼–ç **ï¼šæ‰€æœ‰é…ç½®éƒ½æ¥è‡ªå¤–éƒ¨æ–‡ä»¶
- **å¯è§†åŒ–é…ç½®**ï¼šç”¨æˆ·å‹å¥½çš„Webç•Œé¢
- **å³æ—¶ç”Ÿæ•ˆ**ï¼šå¤§éƒ¨åˆ†é…ç½®ä¿®æ”¹æ— éœ€é‡å¯åº”ç”¨
- **é…ç½®éªŒè¯**ï¼šé¿å…é”™è¯¯çš„é…ç½®å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯
- **ç‰ˆæœ¬æ§åˆ¶å‹å¥½**ï¼šé…ç½®æ–‡ä»¶å¯ä»¥çº³å…¥ç‰ˆæœ¬æ§åˆ¶

### 3.1 åŸºæœ¬é…ç½®

```csharp
var config = new CrawlConfiguration
{
    // å¹¶å‘æ§åˆ¶
    MaxConcurrentTasks = 10,          // æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
    MaxDepth = 3,                     // æœ€å¤§çˆ¬å–æ·±åº¦
    MaxPages = 1000,                  // æœ€å¤§é¡µé¢æ•°
    
    // è¯·æ±‚æ§åˆ¶
    RequestDelay = TimeSpan.FromMilliseconds(500), // è¯·æ±‚é—´éš”
    TimeoutSeconds = 30,              // è¶…æ—¶æ—¶é—´
    
    // åŸŸåé™åˆ¶
    AllowedDomains = new[] { "example.com", "test.com" },
    BlockedPatterns = new[] { "/admin", "/login" },
    
    // è¡Œä¸ºæ§åˆ¶
    RespectRobotsTxt = true,          // éµå®ˆrobots.txt
    FollowRedirects = true            // è·Ÿéšé‡å®šå‘
};
```

### 3.2 é«˜çº§é…ç½®

```csharp
var advancedConfig = new AdvancedCrawlConfiguration
{
    // åŸºç¡€é…ç½®
    MaxConcurrentTasks = 10,
    MaxDepth = 3,
    MaxPages = 5000,
    
    // æ€§èƒ½ä¼˜åŒ–
    MemoryLimitMB = 500,
    MaxQueueSize = 10000,
    EnableCompression = true,
    
    // åçˆ¬è™«åŠŸèƒ½
    EnableAntiBotDetection = true,
    RespectRobotsTxt = true,
    
    // é‡è¯•ç­–ç•¥
    RetryPolicy = new RetryPolicy
    {
        MaxRetries = 3,
        InitialDelay = TimeSpan.FromSeconds(1),
        BackoffMultiplier = 2.0,
        MaxDelay = TimeSpan.FromMinutes(5)
    },
    
    // ä»£ç†è®¾ç½®
    ProxySettings = new ProxySettings
    {
        Enabled = true,
        ProxyUrls = new[] 
        { 
            "http://proxy1.example.com:8080",
            "http://proxy2.example.com:8080" 
        },
        RotationStrategy = ProxyRotationStrategy.RoundRobin
    },
    
    // ç›‘æ§è®¾ç½®
    MonitoringSettings = new MonitoringSettings
    {
        EnableMetrics = true,
        EnableTracing = false,
        MetricsIntervalSeconds = 30
    },
    
    // æ•°æ®æ¸…æ´—
    DataCleaningSettings = new DataCleaningSettings
    {
        RemoveDuplicateContent = true,
        RemoveScriptsAndStyles = true,
        NormalizeText = true,
        MinContentLength = 100
    }
};
```

## 4. é«˜çº§åŠŸèƒ½ç¤ºä¾‹

### 4.1 ä½¿ç”¨é«˜çº§é…ç½®å’Œåçˆ¬è™«åŠŸèƒ½

```csharp
// é«˜çº§çˆ¬è™«ç¤ºä¾‹
var services = new ServiceCollection();
services.AddLogging(builder => builder.AddConsole());

// æ·»åŠ é«˜çº§çˆ¬è™«æœåŠ¡
services.AddAdvancedCrawler(config =>
{
    config.EnableAntiBotDetection = true;
    config.RespectRobotsTxt = true;
    config.RetryPolicy.MaxRetries = 5;
})
.AddCrawlerHttpClient()
.AddFileSystemStorage("advanced_crawler_data")
.AddCrawlerDownloader(useProxies: false)
.AddCrawlerParser()
.AddCrawlerScheduler();

var serviceProvider = services.BuildServiceProvider();
var crawler = serviceProvider.GetRequiredService<CrawlerEngine>();

// ä½¿ç”¨é«˜çº§é…ç½®å¯åŠ¨
await crawler.StartAsync(advancedConfig);
```

### 4.2 åŸŸåè¯·æ±‚èŠ‚æµå’ŒåŠ¨æ€å»¶è¿Ÿè°ƒæ•´

```csharp
// é…ç½®åŸŸåè¯·æ±‚èŠ‚æµ
var services = new ServiceCollection();
services.AddLogging(builder => builder.AddConsole());

// æ·»åŠ å¸¦æœ‰åŸŸåèŠ‚æµåŠŸèƒ½çš„çˆ¬è™«æœåŠ¡
services.AddCrawlerCore()
    .AddFileSystemStorage("throttled_crawler_data")
    .AddCrawlerDownloader()
    .AddCrawlerParser()
    .AddCrawlerScheduler()
    .AddDomainDelayManager(options =>
    {
        // è®¾ç½®é»˜è®¤å»¶è¿Ÿ
        options.DefaultDelay = TimeSpan.FromSeconds(1);
        // è®¾ç½®æœ€å°å’Œæœ€å¤§å»¶è¿Ÿé™åˆ¶
        options.MinDelay = TimeSpan.FromMilliseconds(100);
        options.MaxDelay = TimeSpan.FromSeconds(10);
        // è®¾ç½®è¯·æ±‚ç±»å‹ç‰¹å®šå»¶è¿Ÿ
        options.RequestTypeDelays = new Dictionary<string, TimeSpan>
        {
            { "html", TimeSpan.FromSeconds(1) },
            { "pdf", TimeSpan.FromSeconds(2) },
            { "image", TimeSpan.FromMilliseconds(500) },
            { "api", TimeSpan.FromSeconds(0.5) }
        };
    });

var serviceProvider = serviceProvider = services.BuildServiceProvider();
var crawler = serviceProvider.GetRequiredService<CrawlerEngine>();
var delayManager = serviceProvider.GetRequiredService<IDomainDelayManager>();

// æ‰‹åŠ¨è°ƒæ•´ç‰¹å®šåŸŸåçš„å»¶è¿Ÿ
delayManager.SetDelay("example.com", TimeSpan.FromSeconds(3));
delayManager.SetDelay("example.com", "pdf", TimeSpan.FromSeconds(5));

// å¯åŠ¨çˆ¬è™«
await crawler.StartAsync(config);
```

### 4.3 URLå½’ä¸€åŒ–å’Œä¼˜å…ˆçº§é˜Ÿåˆ—ä¼˜åŒ–

```csharp
// é…ç½®URLå½’ä¸€åŒ–å’Œä¼˜å…ˆçº§é˜Ÿåˆ—
var services = new ServiceCollection();
services.AddLogging(builder => builder.AddConsole());

// æ·»åŠ å¸¦ä¼˜åŒ–åŠŸèƒ½çš„çˆ¬è™«æœåŠ¡
services.AddAdvancedCrawler(config =>
{
    config.EnableUrlNormalization = true;
    config.RespectRobotsTxt = true;
})
.AddCrawlerHttpClient()
.AddFileSystemStorage("optimized_crawler_data")
.AddCrawlerDownloader()
.AddCrawlerParser()
.AddCrawlerScheduler(options =>
{
    // è®¾ç½®ä¼˜å…ˆçº§é˜Ÿåˆ—é€‰é¡¹
    options.HighPriorityDomains = new[] { "example.com", "news.example.com" };
    options.ContentTypePriorities = new Dictionary<string, int>
    {
        { "html", 10 },
        { "pdf", 5 },
        { "image", 1 },
        { "api", 7 }
    };
    // è®¾ç½®ç­‰å¾…æ—¶é—´å½±å“ä¼˜å…ˆçº§çš„é˜ˆå€¼
    options.PriorityIncreaseThreshold = TimeSpan.FromMinutes(5);
});

var serviceProvider = services.BuildServiceProvider();
var crawler = serviceProvider.GetRequiredService<CrawlerEngine>();

// å¯åŠ¨çˆ¬è™«
await crawler.StartAsync(config);
```

### 4.4 åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦

```csharp
// é…ç½®åˆ†å¸ƒå¼çˆ¬è™«
var services = new ServiceCollection();
services.AddLogging(builder => builder.AddConsole());

// æ·»åŠ åˆ†å¸ƒå¼çˆ¬è™«æœåŠ¡
services.AddAdvancedCrawler(config =>
{
    config.EnableDistributedScheduling = true;
    config.MachineName = "crawler-node-1"; // å”¯ä¸€æœºå™¨åï¼Œç”¨äºç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
})
.AddCrawlerHttpClient()
.AddFileSystemStorage("distributed_crawler_data")
.AddCrawlerDownloader()
.AddCrawlerParser()
.AddCrawlerScheduler();

var serviceProvider = services.BuildServiceProvider();
var crawler = serviceProvider.GetRequiredService<CrawlerEngine>();

// å¯åŠ¨çˆ¬è™«
await crawler.StartAsync(config);
```

### 4.5 ä»£ç†å’ŒUser-Agentè½®æ¢

```csharp
// é…ç½®ä»£ç†å’ŒUser-Agentè½®æ¢
var proxyConfig = new AdvancedCrawlConfiguration
{
    MaxConcurrentTasks = 5,
    ProxySettings = new ProxySettings
    {
        Enabled = true,
        ProxyUrls = new[] 
        {
            "192.168.1.100:8080",
            "192.168.1.101:8080:username:password" // å¸¦è®¤è¯çš„ä»£ç†
        }
    }
};

// è‡ªå®šä¹‰User-Agent
var downloader = new AdvancedDownloader(
    logger: logger,
    config: proxyConfig,
    proxyManager: new ProxyManager()
);

// æ·»åŠ è‡ªå®šä¹‰User-Agent
var userAgentService = new RotatingUserAgentService();
userAgentService.AddUserAgent("MyCustomBot/1.0 (+http://mybot.com)");
```

### 4.6 æ•°æ®å¯¼å‡ºåŠŸèƒ½

**æ³¨**ï¼šæ•°æ®å¯¼å‡ºåŠŸèƒ½ç›®å‰å¤„äºå¼€å‘é˜¶æ®µï¼ŒåŸºç¡€æ¥å£å·²å®šä¹‰ï¼Œå®Œæ•´åŠŸèƒ½æ­£åœ¨å®ç°ä¸­ã€‚

```csharp
// æ•°æ®å¯¼å‡ºç¤ºä¾‹ï¼ˆè®¡åˆ’å®ç°ï¼‰
var storage = new FileSystemStorage("crawler_data", logger);
await storage.InitializeAsync();

// è·å–çˆ¬å–çš„æ•°æ®
var results = await storage.GetByDomainAsync("example.com", 100);

// å¯¼å‡ºåŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­
```

### 4.7 ç›‘æ§å’Œå¥åº·æ£€æŸ¥

```csharp
// ç›‘æ§ç¤ºä¾‹
crawler.OnStatusChanged += (s, e) =>
{
    Console.WriteLine($"[{DateTime.Now}] Status: {e.PreviousStatus} -> {e.CurrentStatus}");
    Console.WriteLine($"Message: {e.Message}");
};

crawler.OnUrlDiscovered += (s, e) =>
{
    Console.WriteLine($"Discovered: {e.DiscoveredUrl} (Depth: {e.Depth})");
};

crawler.OnCrawlCompleted += (s, e) =>
{
    var result = e.Result;
    Console.WriteLine($"Completed: {result.Request.Url} " +
                     $"(Status: {result.DownloadResult.StatusCode}, " +
                     $"Time: {result.DownloadResult.DownloadTimeMs}ms)");
};

crawler.OnCrawlError += (s, e) =>
{
    Console.WriteLine($"Error: {e.Request.Url}");
    Console.WriteLine($"Exception: {e.Exception.Message}");
};

// è·å–ç»Ÿè®¡ä¿¡æ¯
var statistics = await crawler.GetStatisticsAsync();
Console.WriteLine($"Total URLs: {statistics.TotalUrlsProcessed}");
Console.WriteLine($"Success Rate: {(double)statistics.SuccessCount / statistics.TotalUrlsProcessed:P2}");
```

## 5. APIå‚è€ƒæ–‡æ¡£

### 5.1 CrawlerEngine - æ ¸å¿ƒçˆ¬è™«å¼•æ“

CrawlerEngineæ˜¯æ•´ä¸ªçˆ¬è™«æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£åè°ƒå’Œç®¡ç†çˆ¬å–ä»»åŠ¡çš„æ‰§è¡Œæµç¨‹ã€‚

#### æ„é€ å‡½æ•°
```csharp
public CrawlerEngine(
    IScheduler scheduler,
    IDownloader downloader,
    IParser parser,
    IStorageProvider storage,
    ILogger<CrawlerEngine> logger,
    IMetadataStore? metadataStore = null,
    AntiBotDetectionService? antiBotService = null,
    AdaptiveRetryStrategy? retryStrategy = null,
    RobotsTxtParser? robotsTxtParser = null,
    CrawlerMetrics? metrics = null,
    IPluginLoader? pluginLoader = null,
    bool enableAutoStop = true,
    TimeSpan? autoStopTimeout = null)
```

**å‚æ•°è¯´æ˜ï¼š**
- `scheduler`: ä»»åŠ¡è°ƒåº¦å™¨ï¼Œè´Ÿè´£ç®¡ç†çˆ¬å–è¯·æ±‚é˜Ÿåˆ—å’Œä¼˜å…ˆçº§
- `downloader`: ä¸‹è½½å™¨ï¼Œè´Ÿè´£ä»ç½‘ç»œè·å–ç½‘é¡µå†…å®¹
- `parser`: è§£æå™¨ï¼Œè´Ÿè´£è§£æä¸‹è½½çš„å†…å®¹å¹¶æå–ä¿¡æ¯
- `storage`: å­˜å‚¨æä¾›å™¨ï¼Œè´Ÿè´£å­˜å‚¨çˆ¬å–ç»“æœå’Œå…ƒæ•°æ®
- `logger`: æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•è¿è¡Œæ—¶ä¿¡æ¯å’Œé”™è¯¯
- `metadataStore`: å…ƒæ•°æ®å­˜å‚¨ï¼Œç”¨äºå­˜å‚¨çˆ¬å–ä»»åŠ¡çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
- `antiBotService`: åæœºå™¨äººæœåŠ¡ï¼Œç”¨äºæ£€æµ‹å’Œåº”å¯¹åçˆ¬æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
- `retryStrategy`: é‡è¯•ç­–ç•¥ï¼Œç”¨äºå¤„ç†ä¸‹è½½å¤±è´¥çš„æƒ…å†µï¼ˆå¯é€‰ï¼‰
- `robotsTxtParser`: Robots.txtè§£æå™¨ï¼Œç”¨äºéµå®ˆç½‘ç«™çš„çˆ¬å–è§„åˆ™ï¼ˆå¯é€‰ï¼‰
- `metrics`: æŒ‡æ ‡æœåŠ¡ï¼Œç”¨äºæ”¶é›†å’ŒæŠ¥å‘Šçˆ¬å–æ€§èƒ½æ•°æ®ï¼ˆå¯é€‰ï¼‰
- `pluginLoader`: æ’ä»¶åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½å’Œç®¡ç†çˆ¬è™«æ’ä»¶ï¼ˆå¯é€‰ï¼‰
- `enableAutoStop`: æ˜¯å¦å¯ç”¨è‡ªåŠ¨åœæ­¢åŠŸèƒ½ï¼Œå½“ä»»åŠ¡é˜Ÿåˆ—ä¸ºç©ºæ—¶è‡ªåŠ¨åœæ­¢ï¼ˆé»˜è®¤ï¼štrueï¼‰
- `autoStopTimeout`: è‡ªåŠ¨åœæ­¢è¶…æ—¶æ—¶é—´ï¼Œå½“ä»»åŠ¡é˜Ÿåˆ—ä¸ºç©ºè¶…è¿‡æ­¤æ—¶é—´æ—¶è‡ªåŠ¨åœæ­¢ï¼ˆé»˜è®¤ï¼š30ç§’ï¼‰

#### æ ¸å¿ƒæ–¹æ³•

**å¯åŠ¨çˆ¬è™«**
```csharp
// ä½¿ç”¨å¹¶å‘ä»»åŠ¡æ•°å¯åŠ¨
public Task StartAsync(int workerCount = 5, string? jobId = null)

// ä½¿ç”¨é«˜çº§é…ç½®å¯åŠ¨
public Task StartAsync(AdvancedCrawlConfiguration config, string? jobId = null)
```

**åœæ­¢çˆ¬è™«**
```csharp
public Task StopAsync(bool saveState = true)
```

**æš‚åœå’Œæ¢å¤çˆ¬è™«**
```csharp
public Task PauseAsync()
public Task ResumeAsync()
```

**æ·»åŠ ç§å­URL**
```csharp
public Task AddSeedUrlsAsync(IEnumerable<string> urls)
```

**è·å–ç»Ÿè®¡ä¿¡æ¯**
```csharp
public Task<Dictionary<string, object>> GetStatisticsAsync()
public Task<CrawlState> GetCurrentCrawlStateAsync()
```

#### äº‹ä»¶
- `OnCrawlCompleted`: çˆ¬å–å®Œæˆäº‹ä»¶
- `OnCrawlError`: çˆ¬å–é”™è¯¯äº‹ä»¶
- `OnUrlDiscovered`: URLå‘ç°äº‹ä»¶
- `OnStatusChanged`: çˆ¬è™«çŠ¶æ€æ”¹å˜äº‹ä»¶

### 5.2 PriorityScheduler - ä¼˜å…ˆçº§è°ƒåº¦å™¨

PrioritySchedulerè´Ÿè´£ç®¡ç†çˆ¬å–è¯·æ±‚çš„é˜Ÿåˆ—å’Œä¼˜å…ˆçº§ï¼Œå¹¶æ”¯æŒåŸŸåè¯·æ±‚èŠ‚æµåŠŸèƒ½ã€‚

#### æ„é€ å‡½æ•°
```csharp
public PriorityScheduler(
    IUrlFilter urlFilter,
    IDomainDelayManager delayManager,
    ILogger<PriorityScheduler> logger)
```

**å‚æ•°è¯´æ˜ï¼š**
- `urlFilter`: URLè¿‡æ»¤å™¨ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦å…è®¸å¤„ç†ç‰¹å®šURL
- `delayManager`: åŸŸåå»¶è¿Ÿç®¡ç†å™¨ï¼Œç”¨äºæ§åˆ¶å¯¹åŒä¸€åŸŸåçš„è¯·æ±‚é¢‘ç‡
- `logger`: æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•è¿è¡Œæ—¶ä¿¡æ¯å’Œé”™è¯¯

#### æ ¸å¿ƒæ–¹æ³•

**æ·»åŠ URLåˆ°é˜Ÿåˆ—**
```csharp
public Task<bool> AddUrlAsync(CrawlRequest request)
public Task<int> AddUrlsAsync(IEnumerable<CrawlRequest> requests)
```

**è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†URL**
```csharp
public async Task<CrawlRequest?> GetNextAsync()
```

**è®°å½•åŸŸåæ€§èƒ½æ•°æ®**
```csharp
public void RecordDomainPerformance(string domain, long downloadTimeMs, bool isSuccess)
```

#### ä¼˜å…ˆçº§è®¡ç®—é€»è¾‘

PrioritySchedulerä½¿ç”¨å¤šç»´åº¦å› ç´ è®¡ç®—URLä¼˜å…ˆçº§ï¼š
1. **åŸºç¡€ä¼˜å…ˆçº§**ï¼šç”±è¯·æ±‚æœ¬èº«æä¾›çš„ä¼˜å…ˆçº§å€¼
2. **æ·±åº¦è°ƒæ•´**ï¼šæ·±åº¦è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šä½ï¼ˆæ¯å¢åŠ 1å±‚æ·±åº¦é™ä½10ä¼˜å…ˆçº§ï¼‰
3. **URLæ¨¡å¼**ï¼šç‰¹å®šå†…å®¹ç±»å‹ï¼ˆå¦‚æ–‡ç« ã€PDFï¼‰è·å¾—æ›´é«˜ä¼˜å…ˆçº§
4. **åŸŸåé‡è¦æ€§**ï¼šé«˜ä¼˜å…ˆçº§åŸŸåè·å¾—é¢å¤–15ä¼˜å…ˆçº§
5. **åŸŸåæ€§èƒ½**ï¼š
   - ä¸‹è½½é€Ÿåº¦è¶Šå¿«ï¼Œä¼˜å…ˆçº§è¶Šé«˜
   - é”™è¯¯ç‡è¶Šé«˜ï¼Œä¼˜å…ˆçº§è¶Šä½
   - è¿ç»­é”™è¯¯çš„åŸŸåè¿›ä¸€æ­¥é™ä½ä¼˜å…ˆçº§
6. **é˜Ÿåˆ—ç­‰å¾…æ—¶é—´**ï¼šé¿å…é¥¥é¥¿ï¼Œé•¿æ—¶é—´ç­‰å¾…çš„è¯·æ±‚ä¼˜å…ˆçº§ä¼šé€æ¸æé«˜

## 6. å­˜å‚¨å’Œæ•°æ®å¤„ç†

### 6.1 æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨

```csharp
// æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨é…ç½®
var fileStorage = new FileSystemStorage("crawler_data", logger);
await fileStorage.InitializeAsync();

// åŸºæœ¬æ“ä½œ
await fileStorage.SaveAsync(crawlResult);
var results = await fileStorage.GetByDomainAsync("example.com");
var specificResult = await fileStorage.GetByUrlAsync("https://example.com/page1");

// ç»Ÿè®¡å’Œå¤‡ä»½
var stats = await fileStorage.GetStatisticsAsync();
await fileStorage.BackupAsync("backup.zip");

// æ¸…ç†æ•°æ®
await fileStorage.ClearAllAsync();
```

### 6.2 SQLiteå­˜å‚¨

```csharp
// SQLiteå­˜å‚¨é…ç½®
var sqliteStorage = new SQLiteStorage("crawler.db", logger);
await sqliteStorage.InitializeAsync();

// é«˜çº§æŸ¥è¯¢
var recentResults = await sqliteStorage.GetByDomainAsync("example.com", 50);
var urlState = await sqliteStorage.GetUrlStateAsync("https://example.com");

// æ•°æ®åº“ç»´æŠ¤
await sqliteStorage.BackupAsync("crawler_backup.db");
var dbStats = await sqliteStorage.GetStatisticsAsync();
```

## 7. è‡ªå®šä¹‰æ‰©å±•

### 7.1 è‡ªå®šä¹‰å†…å®¹æå–å™¨

```csharp
// è‡ªå®šä¹‰æå–å™¨ç¤ºä¾‹
public class ProductExtractor : IContentExtractor
{
    public string Name => "ProductExtractor";

    public Task<ExtractionResult> ExtractAsync(HtmlDocument htmlDocument, DownloadResult downloadResult)
    {
        var result = new ExtractionResult();
        
        // æå–äº§å“ä¿¡æ¯
        var productNodes = htmlDocument.DocumentNode.SelectNodes("//div[@class='product']");
        if (productNodes != null)
        {
            var products = new List<object>();
            foreach (var node in productNodes)
            {
                var product = new
                {
                    Name = node.SelectSingleNode(".//h3")?.InnerText.Trim(),
                    Price = node.SelectSingleNode(".//span[@class='price']")?.InnerText.Trim(),
                    Description = node.SelectSingleNode(".//p[@class='description']")?.InnerText.Trim()
                };
                products.Add(product);
            }
            result.Data["Products"] = products;
        }

        // æå–åˆ†é¡µé“¾æ¥
        var paginationNodes = htmlDocument.DocumentNode.SelectNodes("//a[@class='page-link']");
        if (paginationNodes != null)
        {
            foreach (var node in paginationNodes)
            {
                var href = node.GetAttributeValue("href", "");
                if (!string.IsNullOrEmpty(href))
                {
                    var absoluteUrl = new Uri(new Uri(downloadResult.Url), href).ToString();
                    result.Links.Add(absoluteUrl);
                }
            }
        }

        return Task.FromResult(result);
    }
}

// æ³¨å†Œè‡ªå®šä¹‰æå–å™¨
services.AddSingleton<IContentExtractor, ProductExtractor>();
```

### 7.2 é«˜çº§è§£æå™¨ç‰¹æ€§

AdvancedParseræ”¯æŒå¤šç§å†…å®¹ç±»å‹çš„è§£æå¤„ç†ï¼š

#### æ”¯æŒçš„å†…å®¹ç±»å‹
- **HTML**: å®Œæ•´çš„HTMLæ–‡æ¡£è§£æå’Œå†…å®¹æå–
- **çº¯æ–‡æœ¬**: ç›´æ¥æå–æ–‡æœ¬å†…å®¹ï¼Œæ— éœ€HTMLè§£æ
- **JSON**: ä¸“é—¨å¤„ç†JSONæ•°æ®æ ¼å¼
- **å…¶ä»–ç±»å‹**: ä½œä¸ºåŸå§‹æ•°æ®ä¿å­˜

#### æ€§èƒ½ä¼˜åŒ–
- **ç¼–è¯‘æ—¶æ­£åˆ™è¡¨è¾¾å¼**: ä½¿ç”¨`GeneratedRegexAttribute`åœ¨ç¼–è¯‘æ—¶ç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼ï¼Œæé«˜æ€§èƒ½
- **å®‰å…¨çš„æ–‡æ¡£å¤„ç†**: åˆ›å»ºHTMLæ–‡æ¡£å‰¯æœ¬è¿›è¡Œæ“ä½œï¼Œé¿å…ä¿®æ”¹åŸå§‹æ–‡æ¡£çŠ¶æ€
- **æ™ºèƒ½æ–‡æœ¬æ¸…ç†**: è‡ªåŠ¨ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦ï¼Œæä¾›æ•´æ´çš„æ–‡æœ¬å†…å®¹

#### ä½¿ç”¨ç¤ºä¾‹
```csharp
// è§£æå™¨ä¼šè‡ªåŠ¨æ ¹æ®Content-Typeå¤„ç†ä¸åŒå†…å®¹
var parseResult = await parser.ParseAsync(downloadResult);

// å¯¹äºHTMLå†…å®¹
string htmlTitle = parseResult.Title; // é¡µé¢æ ‡é¢˜
string htmlContent = parseResult.TextContent; // æ¸…ç†åçš„æ–‡æœ¬å†…å®¹

// å¯¹äºçº¯æ–‡æœ¬å†…å®¹
string plainText = parseResult.TextContent; // ç›´æ¥æå–çš„æ–‡æœ¬

// å¯¹äºJSONå†…å®¹
string jsonData = parseResult.ExtractedData["json"] as string; // JSONæ•°æ®
```

**æ³¨**ï¼šAIè¾…åŠ©è§£æåŠŸèƒ½ç›®å‰å¤„äºè®¡åˆ’é˜¶æ®µï¼Œå°šæœªå®ç°ã€‚

### 7.3 è‡ªå®šä¹‰ä¸‹è½½å™¨

```csharp
// è‡ªå®šä¹‰ä¸‹è½½å™¨ç¤ºä¾‹
public class CustomDownloader : IDownloader
{
    private readonly ILogger<CustomDownloader> _logger;
    private readonly HttpClient _httpClient;

    public int ConcurrentRequests { get; set; } = 5;
    public TimeSpan Timeout { get; set; } = TimeSpan.FromSeconds(30);

    public CustomDownloader(ILogger<CustomDownloader> logger, HttpClient httpClient)
    {
        _logger = logger;
        _httpClient = httpClient;
    }

    public async Task<DownloadResult> DownloadAsync(CrawlRequest request)
    {
        try
        {
            // è‡ªå®šä¹‰è¯·æ±‚é€»è¾‘
            var httpRequest = new HttpRequestMessage(HttpMethod.Get, request.Url);
            httpRequest.Headers.Add("Custom-Header", "CustomValue");
            
            var response = await _httpClient.SendAsync(httpRequest);
            var content = await response.Content.ReadAsStringAsync();

            return new DownloadResult
            {
                Url = request.Url,
                Content = content,
                StatusCode = (int)response.StatusCode,
                IsSuccess = response.IsSuccessStatusCode
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Download failed for {Url}", request.Url);
            return new DownloadResult
            {
                Url = request.Url,
                IsSuccess = false,
                ErrorMessage = ex.Message
            };
        }
    }

    public Task InitializeAsync() => Task.CompletedTask;
    public Task ShutdownAsync() => Task.CompletedTask;
}
```

## 8. Webç›‘æ§ç•Œé¢

**æ³¨**ï¼šWebç›‘æ§ç•Œé¢å·²å®ç°åŸºæœ¬åŠŸèƒ½ï¼ŒåŒ…æ‹¬å®æ—¶çŠ¶æ€ç›‘æ§ã€çˆ¬è™«æ§åˆ¶å’Œæ•°æ®å¯è§†åŒ–ã€‚

### 8.1 å¯åŠ¨ç›‘æ§æœåŠ¡ï¼ˆå·²å®ç°ï¼‰

```csharp
// Program.cs for Web Monitor (å·²å®ç°)
var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllersWithViews();
builder.Services.AddSignalR();

// é…ç½®çˆ¬è™«æœåŠ¡
builder.Services.AddAdvancedCrawler()
                .AddCrawlerHttpClient()
                .AddFileSystemStorage("crawler_data")
                .AddCrawlerDownloader()
                .AddCrawlerParser()
                .AddCrawlerScheduler();

var app = builder.Build();

app.UseStaticFiles();
app.UseRouting();

app.MapControllerRoute(
    name: "default",
    pattern: "{controller=Home}/{action=Index}/{id?}");

app.MapHub<CrawlerHub>("/crawlerHub");

app.Run();
```

### 7.2 ç›‘æ§ç•Œé¢åŠŸèƒ½ï¼ˆå·²å®ç°ï¼‰

è®¿é—® `http://localhost:5000` å°†å¯ä»¥çœ‹åˆ°ï¼š

- **å®æ—¶çŠ¶æ€**ï¼šçˆ¬è™«è¿è¡ŒçŠ¶æ€ã€å†…å­˜ä½¿ç”¨ã€è¿è¡Œæ—¶é—´
- **æ€§èƒ½å›¾è¡¨**ï¼šå†…å­˜ä½¿ç”¨ã€URLå¤„ç†æ•°é‡ã€å¤„ç†æ—¶é—´
- **æ´»åŠ¨æ—¥å¿—**ï¼šå®æ—¶æ˜¾ç¤ºçˆ¬è™«æ´»åŠ¨
- **URLåˆ—è¡¨**ï¼šå·²çˆ¬å–çš„URLè¯¦æƒ…
- **æ§åˆ¶é¢æ¿**ï¼šå¯åŠ¨ã€åœæ­¢ã€æš‚åœã€æ¢å¤çˆ¬è™«

## 8. éƒ¨ç½²å’Œè¿ç»´

### 8.1 Dockeréƒ¨ç½²

Dockeréƒ¨ç½²åŠŸèƒ½å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­æä¾›ã€‚

### 8.2 ç”Ÿäº§ç¯å¢ƒé…ç½®

```json
// appsettings.Production.json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft": "Warning",
      "System": "Warning"
    }
  },
  "CrawlerConfig": {
    "MaxConcurrentTasks": 10,
    "MaxDepth": 3,
    "RequestDelay": "00:00:01",
    "StorageType": "SQLite",
    "ConnectionString": "Data Source=/data/crawler.db"
  }
}
```

## 9. æ•…éšœæ’é™¤

### 9.1 å¸¸è§é—®é¢˜è§£å†³

1. **å†…å­˜æ³„æ¼**
   ```csharp
   // å®šæœŸæ¸…ç†
   services.AddSingleton<ObjectPool<HttpClient>>();
   // ç›‘æ§å†…å­˜ä½¿ç”¨
   var memory = GC.GetTotalMemory(false) / 1024 / 1024;
   ```

2. **è¿æ¥è¶…æ—¶**
   ```csharp
   var config = new AdvancedCrawlConfiguration
   {
       TimeoutSeconds = 60,
       RetryPolicy = new RetryPolicy { MaxRetries = 3 }
   };
   ```

3. **åçˆ¬è™«å°é”**
   ```csharp
   config.EnableAntiBotDetection = true;
   config.ProxySettings.Enabled = true;
   config.RequestDelay = TimeSpan.FromSeconds(2);
   ```

### 9.2 æ—¥å¿—é…ç½®

```csharp
// è¯¦ç»†æ—¥å¿—é…ç½®
services.AddLogging(builder =>
{
    builder.AddConsole()
           .AddDebug()
           .AddFile("logs/crawler-{Date}.txt", LogLevel.Information)
           .SetMinimumLevel(LogLevel.Debug);
});
```

## 10. æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **è¿æ¥æ± ä¼˜åŒ–**
   ```csharp
   services.AddHttpClient("CrawlerClient")
           .ConfigurePrimaryHttpMessageHandler(() => new HttpClientHandler
           {
               MaxConnectionsPerServer = 100,
               UseProxy = false
           });
   ```

2. **å†…å­˜ç®¡ç†**
   ```csharp
   // ä½¿ç”¨å¯¹è±¡æ± 
   var httpClientPool = new HttpClientPool(maxClients: 20);
   // å®šæœŸæ¸…ç†ç¼“å­˜
   await storage.ClearAllAsync();
   ```

3. **æ•°æ®åº“ä¼˜åŒ–**
   ```sql
   -- ä¸ºSQLiteåˆ›å»ºç´¢å¼•
   CREATE INDEX IX_CrawlResults_Url ON CrawlResults(Url);
   CREATE INDEX IX_CrawlResults_ProcessedAt ON CrawlResults(ProcessedAt);
   ```

è¿™ä¸ªæ¡†æ¶æä¾›äº†å®Œæ•´çš„ä¼ä¸šçº§çˆ¬è™«è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œé…ç½®å’Œæ‰©å±•ã€‚å»ºè®®ä»åŸºæœ¬é…ç½®å¼€å§‹ï¼Œé€æ­¥å¯ç”¨é«˜çº§åŠŸèƒ½ã€‚